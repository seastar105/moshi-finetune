# data
data:
  eval_data: '' # Optional Fill
  shuffle: true
  train_data: 'seastar105/k-moshi-ft-2ch' # Fill

# model
moshi_paths: 
  hf_repo_id: "seastar105/moshi-eeve-extended"

full_finetuning: true # Activate lora.enable if partial finetuning
lora:
  enable: false # Set to False if full_finetuning is True
  rank: 128
  scaling: 2.
  ft_embed: true # Optional, set to True if you want to finetune the embedding layer
  ft_head: true # Optional, set to True if you want to finetune the head layer

first_codebook_weight_multiplier: 100.
text_padding_weight: .5

# optim
duration_sec: 163.84      # 163.84 seconds = 2048 tokens
batch_size: 1
num_microbatches: 8      # gradient accumulation steps
max_steps: 10000
gradient_checkpointing: true
optim:
  lr: 3e-5
  weight_decay: 0.1
  pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 100
do_eval: false
do_ckpt: true
ckpt_freq: 1000

save_adapters: false # Must be False if full_finetuning is True

run_dir: "runs/local_debug_run"  # Fill

# This part is optional and can be kept commented out
# wandb:
#   project: "" # your wandb project name
#   run_name: "" # your wandb run name
#   key: "" # your wandb api key
#   offline: False
